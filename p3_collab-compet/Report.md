# Project Description

The goal of this project is to train an agent to play tennis in the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment.

In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.
The observation space consists of 8 variables corresponding to the position and velocity of the ball and the agent’s racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.
The task is episodic, and in order to solve the environment, the agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically, after each episode, the rewards that each agent received are added up (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. The maximum of these 2 scores is used as the score for each episode. The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5.

[//]: # (Image References)

[image0]: ./MADDPG_diagram.png
[image1]: ./plot_of_rewards.png

# Learning Algorithm

A general approach for solving a multi-agent problem like this is to use self-play, where starting from a random policy, an agent repeatedly plays against itself (i.e., against an opponent who uses the same policy as the agent) and by trying to maximize its total reward the agent incrementally improves its policy, while its opponent also gets better and better and further challenges the agent. Self-play was at the core of the [AlphaZero](https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd) algorithm that convincingly defeated a world champion program in the games of chess and shogi (Japanese chess) as well as Go.

The algorithm used here for training the Tennis agent is a Multi-Agent Deep Deterministic Policy Gradient (MADDPG), that uses a decentralized actor, centralized critic approach. Please see the original [paper](https://papers.nips.cc/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf) by OpenAI. The following diagram shows an overview of this approach.

<img src="https://github.com/vidavakil/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG_diagram.png" width="600" />

In this approach, the critic that is used only during training is centralized and evaluates the actor's action in a context that involves the states of both agents plus the action that the other agent would have taken in that same context. That is, the critic uses more information, that otherwise is not available to the actor, to evaluate the actor’s policy.

As an example, consider that in the given Tennis environment, the state of the ball is a result of the previous state and action of the other agent and is also going to impact the current state and action of that agent. Rather than trying to learn these implicit causations and correlations to incorporate them in its evaluation of the actor's actions, the critic--that is used only during training--can directly use the other agent's state and action. This is particularly facilitated in a simulation environment. 

# Exploration versus Exploitation

Having random actions/exploration, especially during early phases of training, seems crucial for training. Without that, learning does not seem to happen. Thus, the agent takes random actions in the environment with a probability that starts high, and during an initial period of training is proportionally reduced with each action, until the probability settles at a small value.

Another scheme that allows the otherwise deterministic actor to explore the environment is adding noise to the output of the policy itself. This noise, whether it is a gaussian noise or generated by a Ornstein-Uhlenbeck process, seems to be important especially for preventing the self-playing agent from overfitting, where the agent learns a fixed strategy of hitting the ball the same way as its plays with its opponent (itself) in a mirrored fashion.

To improve the performance and quality of training, Prioritized Experience Replay (PER) is used. PER allows for important experiences/transitions to be sampled with higher probability from the replay buffer which would be otherwise sampled uniformly. The absolute value of the TD-error of experiences is used to prioritize them for sampling, thus allowing experiences that had larger errors to be sampled with higher probability from the replay buffer, and used more often for training, and thus better utilized. 

A previous [ad hoc implementation of prioritized experience replay](https://github.com/vidavakil/deep-reinforcement-learning/blob/master/p2_continuous-control/ddpg_agent.py) had an O(n) complexity (where n is the size of the replay buffer), and used to slow down training significantly. To improve the performance of PER, the priorities (derived from TD-errors) are now stored in a [sum-tree](https://arxiv.org/pdf/1511.05952.pdf). This reduces the complexity of the critical step of sampling from the replay buffer to O(log n). PER requires two parameters, ```alpha``` and ```beta```, that are typically annealed to 1.0 over the course of training. At the start of training the replay buffer is sampled more uniformly, and as the training progresses, more weight is given to experiences with large errors. For this project, the value of ```alpha``` is kept fixed to avoid having to recompute the priorities stored in the sum-tree. The ```beta``` hyper-parameter however is annealed towards 1.0. Still, without a more thorough and systematic tuning of the hyper-parameters, it's not clear if/how the prioritized replay buffer is improving the performance of training, as even with fixed hyper-parameters, different training trials seem to complete in number of episodes that are different by as many as 1000 episodes.

# Model Architecture
The way the states and actions of a given agent versus that of other agents are packaged and fed to the agent’s critic network are important. Critic networks typically have separate input bundles for the state and actions. When deciding on where to feed the action of the other agent(s) into an agent's critic, it seems more logical--as also confirmed by experiments--that the action of the other agent better be packaged together with the combined state of the two agents, than being packaged with the action of the agent itself. 

The critic network used in this project is a fully connected network with two hidden layers. The first layer has 256 units and its input is a concatenation of the agent's own state, the opponent's state, and the opponent's action (that it would have taken in same combined state). The second layer has 128 units, and its input is a concatenation of the output of the first layer and the agent's own action. The hidden layers are followed by ```Leaky-ReLU``` activation units. The final layer has a single output that is the estimated value of the overall-state/action pair.

The actor network used in this project is a fully connected network with two hidden layers. The first layer has 256 units and its input is the actor's own state. The second hidden layer has 128 units. The hidden layers are followed by ```Leaky-ReLU``` activation units. The output layer has two units, one for each of the two actions proposed by the actor for the given input state.

Default initialization is used for the weights and biases in the two networks.

A soft-update algorithm is used to update the target actor/critic networks after each update of the local actor/critic networks. Experimenting with the ```tau``` parameter of the soft-update algorithm shows that increasing it over the course of training towards a maximum value of 0.5 improves the performance of learning. 

# Hyperparameters
The MADDPG algorithm used and implemented for this project has a number of hyper-parameters that can be tuned for faster convergence and more stable training. These include the following:

- ```train_every```: the number of experiences that are explored in the sampling phase and added to the replay buffer before switching to the learning phase.
- ```train_steps```: the number of training steps in each learning phase. In each training step, a single mini-batch of experiences randomly sampled from the replay buffer is used for one step of training the local networks and immediately updating the target networks using a soft-update algorithm.
- ```buffer_size```: the size of the circular replay buffer. The implementation of sum-tree requires this to be a power of two
- ```batch_size```: the mini-batch size used in training the local networks
- ```lr_actor```: the learning rate used for training the local actor network
- ```lr_critic```: the learning rate used for training the local critic network
- ```weight_decay```: the L2 regularization parameter for critic’s network
- ```gamma```: the discount factor in the update rule of the critic (Q-Learning) that discounts future rewards
- ```tau```: used for updating each of the two actor and critic target networks as a weighted average of the old target network and the updated local network.
```target_network_parameters = TAU * - local_network_parameters + (1.0 - TAU) * target_network_parameters```
- ```tau_increase```: the rate at which ```tau``` is increased
- ```add_noise```: enable adding noise to the actor’s output
- ```use_ounoise```: enables using a Ornstein-Uhlenbeck process versus gaussian noise for adding to the actor’s output action
- ```random_action_period```: number of steps in early training where the agent takes random actions with a probability that is proportionally reduced as the period concludes
- ```minimum_random_action_prob```: the lowest probability used by the agent to take random actions in the environment
- ```noise_theta```, ```noise_sigma```: ```theta``` and ```sigma``` parameters of the Ornstein-Uhlenbeck process
- ```sigma_decay```, ```theta_decay```: used to multiplicatively decay the corresponding noise parameters at the end of each collective episode, to reduce the amount of exploration over the course of training
- ```prioritized_replay```: enables PER
- ```maximum_error```: when an experiences is first added to PER, it is assigned the highest value found in the PER. If PER is empty, maximum_error is used.
- ```alpha```: used to control the amount of uniformity when sampling from PER. An ```alpha = 0.0``` means that experiences are sampled uniformly, while 1.0 means true prioritization.
- ```beta_start```: the staring value for the beta hyper-parameter of PER. beta controls another adjusts for sampling from the prioritized distribution of experiences instead of the real distribution.
- ```beta_decay```: ```beta``` is increased as training makes progress. ```beta_decay``` decays ```(1 - beta)```.
- ```beta_end```: the cap for ```beta```.

Using the following values for the above hyper-parameters, an average score of 0.522 (over the latest 100 episodes) was achieved after 2208 episodes of training (an episode ends when the environment returns a done = True in response to the last action taken by an agent).
```
train_every = 20
train_steps = 10 
buffer_size = 2^20
batch_size = 512
lr_actor = 1e-4
lr_critic = 1e-3
weight_decay = 0.00001
gamma = 0.99
tau = 0.001
tau_increase = 1.0001
add_noise = True 
use_ounoise = True
random_action_period = 2000
minimum_random_action_prob = 0.01
noise_theta = 0.15
noise_sigma = 0.2
theta_decay = 0.99
sigma_decay = 0.99
prioritized_replay = False
epsilon_error = 1e-7
maximum_error = 1e4
alpha = 0.6
beta_start = 0.4
beta_decay = 0.99
beta_end = 1.0
```

# Plot of Rewards 

After each episode, the raw rewards received by each agent are added up to compute a score for the agent. This yields 2 (potentially different) scores. The maximum of these 2 scores is then taken, yielding a single score for each episode.

The following shows the plot of rewards per episode of training the agents with a goal of reaching an average score of +0.5. An average score of 0.522 (over the latest 100 episodes) was achieved after 2208 episodes. Checkpoints for the actor and critic networks are available in files ```checkpoint_actor.pth``` and ```checkpoint_critic.pth```.

<img src="https://github.com/vidavakil/deep-reinforcement-learning/blob/master/p3_collab-compet/plot_of_rewards.png" width="600" />

# Ideas for Future Work

Training RL agents is tricky, and as they say, seems to be like an art form. Tuning hyper-parameters seem to be the most challenging part of training RL algorithms. As an example, many of the experiments that I have run for this project, and others in the Udacity Deep RL Nanodegree program, would not show any sign of learning for many steps of training, and even judging the learning by watching the agents playing against each other can be hard. For future work, I’d like to explore ways to help automate tuning hyper-parameters of RL algorithms. Exploring more complex multi-agent environments, where there is a combination of competition and collaboration, and where communication between agents becomes an important tool that has to emerge from their interactions, is a fascinating area that I would definitely be further exploring. 
